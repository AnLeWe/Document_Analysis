{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83cf750-33a5-4457-9a8c-4b13b4c44cd4",
   "metadata": {},
   "source": [
    "## Document Analysis: Computational Methods - Summer Term 2025\n",
    "### Lectures: Jun.-Prof. Dr. Andreas Spitz\n",
    "### Tutorials: Julian Schelb\n",
    "\n",
    "Buket Sak, Anna Werner, Yu Zeyuan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675e5c2-c60f-43e9-9076-a3bf7a36ffa1",
   "metadata": {},
   "source": [
    "# Exercise 02\n",
    "\n",
    "You will learn about:\n",
    "    \n",
    "- The Brown Corpus\n",
    "- Part of Speech (POS) tagging\n",
    "- Unigram and Bigram tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd621e3-4f8e-4fbb-88e2-8910fca352ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b77e4-4f2e-4441-85e3-5a007faa73c7",
   "metadata": {},
   "source": [
    "## Task 1 - The Brown Corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbabdb-28ad-414c-b4fd-5d8e244e793a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42de428-53b6-4a0e-8311-3ecd0dafa6d7",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "In the following, we will use the _Brown Corpus_. In one or two sentences, describe what the _Brown Corpus_ is and how it can be used for POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0ae3b-f490-4174-bc99-b02760f5e197",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 50-100 words\n",
    "Brown Corpus has tagset where we can make use of while doing part of speech tagging.\n",
    "\n",
    "The Brown Corpus includes English texts from 15 different categories, such as news, fiction etc. It can be used for POS tagging because it contains manually tagged sentences, allowing researchers to train and evaluate POS taggers effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09200db1-e168-4173-a85e-92c5630ddabc",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "We start by analyzing which tags occur in the brown corpus. For this, you should extract the `tagged_words` first. Then\n",
    "\n",
    "1. List the first 20 entries and\n",
    "2. then list the ten most common tags in the category `news`.\n",
    "\n",
    "In the lecture, we use the Brown Corpus POS tags (default, i.e., `tagset=None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48b5e71-e2ed-4b57-af65-f352d0873632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/buketsak/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfca02e-0ed1-46ca-a0e7-b37b819d7c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS')]\n",
      "\n",
      " [('NN', 13162), ('IN', 10616), ('AT', 8893), ('NP', 6866), (',', 5133), ('NNS', 5066), ('.', 4452), ('JJ', 4392), ('CC', 2664), ('VBD', 2524)]\n"
     ]
    }
   ],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)\n",
    "from collections import Counter\n",
    "tagged_words = brown.tagged_words()\n",
    "print(tagged_words[0:20])\n",
    "\n",
    "# Get tagged words only from the 'news' category\n",
    "brown.tagged_words(categories='news')\n",
    "# Extract only the tags\n",
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "# Count frequency of tags\n",
    "tag_counts = Counter(tags)\n",
    "most_common_tags = tag_counts.most_common(10)\n",
    "print(\"\\n\", most_common_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02186ccd-c25c-4788-8cd3-d4bc636ad93e",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "In the previous part, you should get ten different POS tags. For each tag, what does it stand for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9703d-5e72-40d9-a220-ccd2bac2e5f9",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected: explanation for each tag\n",
    "\n",
    "NN: noun singular\n",
    "\n",
    "IN: in preposition\n",
    "\n",
    "AT: article\n",
    "\n",
    "NP: Proper noun?\n",
    "\n",
    "NNS: Noun plural\n",
    "\n",
    "JJ: Adjective\n",
    "\n",
    "CC: Coordinating conjunction \n",
    "\n",
    "VBD: Verb, past tense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6adb6-1284-4ad3-87b7-b2d3ba5ffbea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab5f72-7433-4338-8f7f-2ab002c03074",
   "metadata": {},
   "source": [
    "## Task 2 - POS Tagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c060a2-003d-47a7-a929-e08c940b7389",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "Use a Unigram tagger, trained on the Brown corpus, to tag the example sentence from the Penn treebank (see also https://www.nltk.org/_modules/nltk/corpus/reader/tagged.html)\n",
    "\n",
    "For which words does it completely fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a5acd4-6ab3-494b-a08b-6992b2a0919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/buketsak/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/buketsak/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d492dec-b7b7-4c86-92e8-f3873221dff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken']\n"
     ]
    }
   ],
   "source": [
    "treebank_test = list(nltk.corpus.treebank.words()[0:20])\n",
    "print(treebank_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7eed60-2a56-4ce9-a0af-cae2eccb987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NP'), ('Vinken', None), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'AT'), ('board', 'NN'), ('as', 'CS'), ('a', 'AT'), ('nonexecutive', None), ('director', 'NN'), ('Nov.', 'NP'), ('29', 'CD'), ('.', '.'), ('Mr.', 'NP'), ('Vinken', None)]\n"
     ]
    }
   ],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)\n",
    "trained_data = brown.tagged_sents()\n",
    "unigram_tagger = UnigramTagger(trained_data)\n",
    "tagged_sent = unigram_tagger.tag(treebank_test)\n",
    "\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b2244-39c2-4139-90c2-f63b8f1004e0",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected list of words\n",
    "\n",
    "For the proper noun Vinken and the word nonexecutive, the UnigramTagger completely fails and returns None because these words were not present in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49def5-67ab-40e2-a795-28e96acddb1f",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Compare the tags with the reference tags from the Penn treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "170ceed1-3121-4580-b277-223c381167ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Predicted  Gold\n",
      "--------------------------------\n",
      "Pierre          NP         NNP\n",
      "Vinken          NN         NNP\n",
      ",               ,          ,\n",
      "61              CD         CD\n",
      "years           NNS        NNS\n",
      "old             JJ         JJ\n",
      ",               ,          ,\n",
      "will            MD         MD\n",
      "join            VB         VB\n",
      "the             AT         DT\n",
      "board           NN         NN\n",
      "as              CS         IN\n",
      "a               AT         DT\n",
      "nonexecutive    NN         JJ\n",
      "director        NN         NN\n",
      "Nov.            NP         NNP\n",
      "29              CD         CD\n",
      ".               .          .\n",
      "Mr.             NP         NNP\n",
      "Vinken          NN         NNP\n"
     ]
    }
   ],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import DefaultTagger \n",
    "# Get the first 20 words and gold tags\n",
    "treebank_test = list(treebank.words()[0:20])\n",
    "treebank_gold = list(treebank.tagged_words()[0:20])\n",
    "\n",
    "# Train UnigramTagger on the Brown corpus with Backoff\n",
    "backoff = DefaultTagger('NN')  # fallback tagger\n",
    "unigram_tagger = UnigramTagger(trained_data, backoff=backoff)\n",
    "# Predict tags\n",
    "tagged_sent = unigram_tagger.tag(treebank_test)\n",
    "\n",
    "print(f\"{'Word':15} {'Predicted':10} {'Gold'}\")\n",
    "print(\"-\" * 32)\n",
    "for (word_pred, pred_tag), (word_gold, gold_tag) in zip(tagged_sent, treebank_gold):\n",
    "    print(f\"{word_pred:15} {pred_tag:10} {gold_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13abc46-aebe-41d1-ba16-29edc535e942",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected comparison for each tag\n",
    "\n",
    "So we’re evaluating how well a UnigramTagger trained on the Brown corpus performs when tagging a sentence from the Penn Treebank. For words that are unseen, the tagger falls back to the default tag 'NN'. However, this can lead to inaccurate results. For example, the word “nonexecutive” is tagged as 'NN', while its correct tag is 'JJ', and “Vinken” is a proper noun, but is also incorrectly tagged as 'NN'.\n",
    "There are also differences in tagsets between the Brown and Penn Treebank corpora. For instance:\n",
    "\n",
    "AT in Brown, DT in TreeBank.\n",
    "\n",
    "NP in Brown, NNP in TreeBank.\n",
    "\n",
    "CS in Brown, IN in TreeBank. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9781814-a8ac-4d0f-a884-189cb65492cd",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Now train \n",
    " 1. a Unigram tagger,\n",
    " 2. a Bigram tagger,\n",
    " 3. and a Brill tagger (using rule brill24)\n",
    " \n",
    "with a subset of the Brown Corpus. This might take 1-2 minutes.\n",
    "\n",
    "Then, validate and compare their performance on a different subset of the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f019a5-0973-4268-bc65-e819a1722be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.tag.brill_trainer import BrillTaggerTrainer\n",
    "from nltk.tag.brill import brill24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c764526-7add-4c98-b216-fb57e9a8fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cutoff = 20000\n",
    "brown_sents_train = brown.tagged_sents()[0:n_cutoff] # training corpus\n",
    "brown_sents_test = brown.tagged_sents()[n_cutoff:] # reference corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2e1d093-a53c-45dc-b4c5-78b20afb3bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBL train (fast) (seqs: 20000; tokens: 430477; tpls: 24; min score: 2; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 247221 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      "32083209   1  13  | TO->IN if Pos:NN@[1]\n",
      "10801080   0   6  | NN->AT if Word:the@[0]\n",
      " 252 252   0   1  | NN->AT if Word:a@[0]\n",
      " 239 249  10   6  | CS->QL if Word:as@[2]\n",
      " 136 195  59  70  | CS->WPS if Pos:NN@[-1] & Pos:NN@[1]\n",
      "  72  72   0   1  | NN->PP$ if Word:his@[0]\n",
      "  68  69   1   0  | EX->RB if Pos:NN@[1]\n",
      "  62  72  10   0  | IN-TL->IN if Pos:NN@[1,2]\n",
      "  61  62   1   2  | QL->AP if Word:than@[1]\n",
      "  60  60   0   0  | NN->. if Word:.@[-1,0]\n",
      "  60  97  37 157  | NN->VBZ if Word:that@[-1] & Pos:WPS@[-1]\n",
      "  60  60   0   0  | JJ->ABL if Word:such@[0] & Word:a@[1] & Pos:AT@[1]\n",
      "  57  93  36   9  | JJ->NN if Pos:AT@[-1] & Pos:IN@[1]\n",
      "  57  58   1   4  | NP-TL->NP if Pos:AT@[-1] & Pos:NN@[1]\n",
      "  56  56   0  24  | QL->AP if Pos:AT@[-1] & Pos:NN@[1]\n",
      "  51  51   0   5  | NN->PPO if Word:him@[0,1]\n",
      "  49  49   0   0  | NN->DT if Word:this@[0]\n",
      "  45  45   0   0  | NN->PP$ if Word:their@[0]\n",
      "  44  44   0   0  | IN->TO if Word:to@[0] & Word:be@[1] & Pos:BE@[1]\n",
      "  43  43   0   0  | NN->AT if Word:an@[0]\n",
      "  42  42   0  47  | WPS->CS if Pos:NN@[1]\n",
      "  42  42   0  21  | CS->WPS if Word:that@[0] & Word:is@[1] & Pos:BEZ@[1]\n",
      "  42  56  14   0  | RP->IN if Word:out@[0] & Word:of@[1] & Pos:IN@[1]\n",
      "  41  44   3   3  | CS->WPS if Pos:NN@[-1] & Pos:MD@[1]\n",
      "  40  44   4  55  | NN->RB if Pos:QL@[-1] & Pos:CS@[1]\n",
      "  68  69   1 113  | QL->RB if Pos:NN@[1]\n",
      "  39  39   0   0  | AT-TL->AT if Pos:NN@[1]\n",
      "  37  37   0   1  | NN->PPO if Word:me@[0,1]\n",
      "  35  61  26  41  | CS->RB if Word:of@[1]\n",
      "  37  37   0   8  | RB->DT if Word:that@[0]\n",
      "  32  32   0   0  | NN->WDT if Word:which@[0]\n",
      "  30  38   8  11  | WPS->CS if Pos:NNS@[-1] & Pos:VBZ@[1]\n",
      "  30  30   0   1  | NN->IN if Word:of@[0]\n",
      "  28  28   0   7  | NN->PPO if Word:it@[0]\n",
      "  28  28   0   0  | NN->PPO if Word:them@[0]\n",
      "  27  27   0   9  | NN->PPSS if Word:they@[-1,0]\n",
      "  27  27   0   1  | CS->WPS if Word:that@[0] & Word:has@[1] & Pos:HVZ@[1]\n",
      "  26  29   3   0  | NR-TL->JJ-TL if Pos:NN@[1]\n",
      "  26  26   0   0  | NN->WDT if Word:what@[0]\n",
      "  26  27   1   0  | AT->QL if Word:no@[0] & Word:longer@[1] & Pos:JJR@[1]\n",
      "  25  27   2   1  | CS->QL if Word:well@[1]\n",
      "  24  78  54  61  | NN->VB if Pos:IN@[-1] & Pos:AT@[1]\n",
      "  48  48   0   0  | IN->TO if Pos:RB@[-1] & Pos:VB@[1]\n",
      "  24  27   3  10  | QL->CS if Word:that@[0,1]\n",
      "  24  24   0   0  | NN->PP$ if Word:its@[0]\n",
      "  24  26   2   0  | JJR->RBR if Word:no@[-1] & Pos:QL@[-1]\n",
      "  23  23   0   8  | RB->AP if Word:more@[0]\n",
      "  22  23   1   0  | IN->CS if Pos:PPSS@[1]\n",
      "  22  22   0   2  | RB->CS if Word:as@[0]\n",
      "  21  21   0   0  | WPS->CS if Pos:AT@[1]\n",
      "  21  21   0   0  | CD->CD-TL if Word:John@[-1]\n",
      "  21  22   1   2  | IN->CS if Word:he@[1]\n",
      "  21  41  20   0  | VBD->VBN if Word:by@[1]\n",
      "  21  23   2   0  | IN->RP if Word:on@[0] & Word:to@[1] & Pos:IN@[1]\n",
      "  21  21   0   0  | RB->CS if Word:so@[0] & Word:that@[1] & Pos:CS@[1]\n",
      "  20  22   2   0  | CD->PN if Word:no@[-1]\n",
      "  19  20   1   0  | NP->NP-TL if Pos:NN-TL@[-1] & Pos:CD-TL@[1]\n",
      "  19  20   1   1  | VB->NN if Word:in@[-1]\n",
      "  19  19   0   5  | NN->DT if Word:that@[0]\n",
      "  19  19   0   1  | NN->DTI if Word:any@[0]\n",
      "  19  19   0   0  | NN->DTS if Word:these@[0]\n",
      "  19  19   0   0  | NN->PP$ if Word:our@[0]\n",
      "  19  36  17  13  | JJ->QL if Word:little@[0] & Word:a@[-1] & Pos:AT@[-1]\n",
      "  19  22   3   0  | CS->IN if Word:as@[0] & Word:to@[1] & Pos:IN@[1]\n",
      "  18  18   0   2  | PP$->PPO if Pos:.@[1]\n",
      "  18  18   0   9  | IN-HL->IN if Pos:NN@[1,2,3]\n",
      "  18  21   3  40  | NN->JJ if Word:so@[-1]\n",
      "  18  18   0   8  | NN->PPO if Word:us@[0,1]\n",
      "  17  18   1   0  | TO->IN if Pos:PPO@[1]\n",
      "  17  32  15   8  | CS->DT if Pos:VB@[-1] & Pos:NN@[1]\n",
      "  17  20   3  14  | NN->JJ if Word:how@[-1]\n",
      "  17  17   0   0  | NN->, if Word:,@[0]\n",
      "  17  17   0   1  | NN->ABN if Word:all@[0]\n",
      "  17  17   0   1  | NN->PP$ if Word:your@[0]\n",
      "  16  18   2   3  | WPS->CS if Pos:PPS@[1]\n",
      "  16  19   3   6  | NN-HL->NN if Pos:NN@[1,2,3]\n",
      "  16  16   0   0  | VBN->VBD if Pos:NP@[-2] & Pos:RB@[-1]\n",
      "  16  16   0   0  | :->:-HL if Word:feed@[-1]\n",
      "  16  16   0   0  | VB->VB-HL if Pos::-HL@[1]\n",
      "  16  16   0   0  | TO->TO-HL if Pos:VB-HL@[1]\n",
      "  16  16   0   0  | WRB->WRB-HL if Pos:TO-HL@[1]\n",
      "  16  16   0   1  | CS->QL if Word:much@[1,2]\n",
      "  16  16   0   4  | NN->CD if Word:million@[0,1]\n",
      "  16  16   0  20  | NN->IN-TL if Word:Of@[0,1]\n",
      "  16  16   0   3  | NN->NN-TL if Word:Service@[0,1]\n",
      "  15  15   0  29  | QL->CS if Pos:AT@[1]\n",
      "  15  17   2   1  | VBN->VBD if Pos:NNS@[-1] & Pos:AT@[1]\n",
      "  15  17   2   0  | VBN->VBD if Pos:PPS@[-2] & Pos:RB@[-1]\n",
      "  15  53  38  29  | IN->RP if Word:.@[1]\n",
      "  15  15   0   0  | JJ->JJ-HL if Word:birth@[1]\n",
      "  15  15   0   0  | AT->AT-HL if Pos:JJ-HL@[1]\n",
      "  15  16   1   5  | NN->NN-TL if Word:Society@[0,1]\n",
      "  15  15   0   3  | NN->CD if Word:one@[0]\n",
      "  15  15   0   0  | NN->DTS if Word:those@[0]\n",
      "  15  15   0   0  | NN->JJ-TL if Word:New@[0]\n",
      "  15  15   0   3  | NN->PPO if Word:you@[0]\n",
      "  15  22   7   0  | RB->AP if Word:much@[0]\n",
      "  15  15   0   0  | NN->NN-HL if Word:birth@[0] & Word:new@[-1] & Pos:JJ-\n",
      "                  |   HL@[-1]\n",
      "  15  15   0   0  | CS->RB if Word:before@[0] & Word:,@[1] & Pos:,@[1]\n",
      "  15  16   1   1  | CS->WPS if Word:that@[0] & Word:had@[1] & Pos:HVD@[1]\n",
      "  15  15   0   8  | CS->WPS if Word:that@[0] & Word:was@[1] & Pos:BEDZ@[1]\n",
      "  15  21   6   0  | RP->IN if Word:over@[0] & Word:the@[1] & Pos:AT@[1]\n",
      "  14  18   4  13  | QL->RB if Pos:,@[1]\n",
      "  14  15   1   0  | DTX->CC if Pos:CC@[1,2,3]\n",
      "  14  19   5   5  | PPO->PP$ if Pos:VBD@[-1] & Pos:NN@[1]\n",
      "  14  21   7   0  | VBN->VBD if Pos:NN@[-1] & Pos:AT@[1]\n",
      "  14  14   0   2  | NN->NN-TL if Word:Administration@[0,1]\n",
      "  14  14   0   0  | NN->AP if Word:other@[0]\n",
      "  14  15   1   0  | IN->RP if Word:on@[0] & Word:,@[1] & Pos:,@[1]\n",
      "  13  14   1  10  | CS->RB if Pos:.@[1]\n",
      "  13  13   0  10  | QL->RB if Pos:.@[1]\n",
      "  13  14   1   2  | NP-HL->NP if Pos:NN@[1,2,3]\n",
      "  13  15   2  13  | NN->NP-TL if Word:New@[-1]\n",
      "  13  14   1   0  | CS->QL if Word:far@[1]\n",
      "  13  16   3   2  | IN->RP if Word:from@[1]\n",
      "  13  16   3   1  | RB->QL if Word:far@[1]\n",
      "  13  13   0   2  | NN->CS if Word:as@[0]\n",
      "  13  13   0   0  | NN->JJ if Word:public@[0] & Word:a@[-1] & Pos:AT@[-1]\n",
      "  13  14   1   0  | PPO->PPS if Word:it@[0] & Word:was@[1] & Pos:BEDZ@[1]\n",
      "  13  28  15   0  | RP->IN if Word:down@[0] & Word:the@[1] & Pos:AT@[1]\n",
      "  12  12   0   0  | BEZ->BEZ-HL if Pos:NN-HL@[-1]\n",
      "  12  13   1   2  | AT->AT-TL if Pos:IN-TL@[1]\n",
      "  12  12   0   7  | JJ-HL->JJ if Pos:NN@[1,2]\n",
      "  12  20   8   0  | CC->CC-TL if Pos:NN-TL@[-1] & Pos:NN@[1]\n",
      "  12  25  13   8  | NN->NN-TL if Pos:CC-TL@[-2,-1]\n",
      "  12  16   4   1  | CS->IN if Pos:AP@[-1] & Pos:NN@[1]\n",
      "  12  25  13  29  | NN->JJ if Pos:RB@[-1] & Pos:NN@[1]\n",
      "  12  13   1   0  | NN->VBN if Word:well@[-1]\n",
      "  12  12   0   3  | NN->NP if Word:Mr.@[-1,0]\n",
      "  12  12   0   1  | NN->NP if Word:Piazza@[-1,0]\n",
      "  12  12   0   3  | NN->JJ if Word:own@[0,1]\n",
      "  12  12   0   5  | NN->NN-TL if Word:Association@[0,1]\n",
      "  12  12   0   6  | NN->NN-TL if Word:Church@[0,1]\n",
      "  12  31  19   0  | NP->NP-TL if Pos:IN@[-1] & Pos:NN-TL@[1]\n",
      "  12  12   0   0  | NN->AP if Word:many@[0]\n",
      "  12  12   0   0  | IN->RP if Word:in@[0] & Word:on@[1] & Pos:IN@[1]\n",
      "  12  13   1   0  | JJ->ABL if Word:such@[0] & Word:an@[1] & Pos:AT@[1]\n",
      "  12  13   1   0  | JJ->NN if Word:right@[0] & Word:to@[1] & Pos:TO@[1]\n",
      "  11  12   1   0  | :->:-HL if Pos:DOZ@[-1]\n",
      "  11  11   0   2  | NN->CD-TL if Pos:.@[-1]\n",
      "  11  11   0   2  | .->:-TL if Pos:CD-TL@[1]\n",
      "  11  12   1   0  | DOZ->DOZ-HL if Pos::-HL@[1]\n",
      "  11  12   1   0  | PPS->PPS-HL if Pos:DOZ-HL@[1]\n",
      "  11  12   1   0  | WDT->WDT-HL if Pos:PPS-HL@[1]\n",
      "  11  31  20   1  | NP->NP-TL if Pos:AT@[-1] & Pos:NN-TL@[1]\n",
      "  11  13   2   1  | VBN->VBD if Pos:NNS@[-1] & Pos:NN@[1]\n",
      "  11  11   0   0  | VBD->VBN if Pos:BEDZ@[-2] & Pos:RB@[-1]\n",
      "  11  13   2   0  | VBN->VBD if Pos:PPSS@[-2] & Pos:RB@[-1]\n",
      "  11  19   8   0  | VBD->VBN if Pos:HV@[-3,-2,-1]\n",
      "  11  12   1   0  | CD->PN if Word:No@[-1]\n",
      "  11  12   1   2  | NN->NN-TL if Word:Coast@[-1,0]\n",
      "  11  11   0   0  | NN->AP if Word:few@[0,1]\n",
      "  11  11   0   0  | NN->AP if Word:same@[0,1]\n",
      "  11  11   0   3  | NN->AP if Word:much@[0]\n",
      "  11  11   0   1  | NN->DT if Word:another@[0]\n",
      "  11  11   0   0  | PP$->PPO if Word:her@[0] & Word:,@[1] & Pos:,@[1]\n",
      "  11  21  10   0  | PPO->PPS if Word:it@[0] & Word:is@[1] & Pos:BEZ@[1]\n",
      "  14  14   0   0  | PPS->PPO if Word:of@[-1] & Pos:IN@[-1]\n",
      "  10  12   2   0  | NNS-HL->NNS if Pos:NN@[1,2]\n",
      "  10  10   0   0  | AT->NN if Pos:``@[-1] & Pos:''@[1]\n",
      "  10  11   1   0  | IN->TO if Pos:RP@[-1] & Pos:VB@[1]\n",
      "  10  10   0   0  | VBD->VBN if Pos:HVD@[-2] & Pos:RB@[-1]\n",
      "  10  10   0  11  | NN->NP-TL if Word:West@[-1]\n",
      "  10  15   5   6  | IN->RP if Word:by@[1]\n",
      "  10  13   3   2  | NN->NP if Word:John@[-1,0]\n",
      "  10  10   0   0  | NN->DT if Word:each@[0]\n",
      "  10  10   0   0  | RP->IN if Word:for@[0]\n",
      "  10  10   0   0  | JJ->NN if Word:public@[0] & Word:the@[-1] & Pos:AT@[-1]\n",
      "  10  10   0   6  | NN-TL->NP if Word:St.@[0] & Word:Louis@[1] & Pos:NP@[1]\n",
      "  10  11   1   0  | PPO->PPS if Word:it@[0] & Word:would@[1] & Pos:MD@[1]\n",
      "   9   9   0   7  | IN-TL->NN-TL if Pos:AT-TL@[-1]\n",
      "   9  33  24  60  | VB->NN if Pos:IN@[-1] & Pos:AT@[1]\n",
      "  10  10   0   0  | NN->ABX if Word:both@[0]\n",
      "   9   9   0   0  | AP->QL if Word:becomes@[-1]\n",
      "   9  10   1   4  | NN->VB if Word:you@[-1]\n",
      "   9   9   0   0  | NP->NP-TL if Word:College@[1]\n",
      "   9  12   3   0  | QL->RB if Word:from@[1]\n",
      "   9   9   0   8  | NN->NN-TL if Word:President@[-1,0]\n",
      "   9   9   0   2  | NN->NP if Word:God@[-1,0]\n",
      "   9   9   0   1  | NN->NP if Word:Mrs.@[-1,0]\n",
      "   9   9   0  15  | NN->PPSS if Word:I@[-1,0]\n",
      "   9  10   1   2  | NN->NN-TL if Word:Company@[0,1]\n",
      "   9   9   0   0  | NN->IN if Word:into@[0]\n",
      "   9  10   1   0  | IN->RP if Word:in@[0] & Word:bring@[-1] & Pos:VB@[-1]\n",
      "   9   9   0   0  | WPS->DT if Word:that@[0] & Word:,@[-1] & Pos:,@[-1]\n",
      "   9  33  24   0  | CS->IN if Word:before@[0] & Word:the@[1] & Pos:AT@[1]\n",
      "   9  11   2   0  | IN->RP if Word:in@[0] & Word:,@[1] & Pos:,@[1]\n",
      "   9   9   0   0  | IN->RP if Word:in@[0] & Word:for@[1] & Pos:IN@[1]\n",
      "   9  10   1   0  | RB->IN if Word:along@[0] & Word:the@[1] & Pos:AT@[1]\n",
      "   8   9   1   0  | AT->RB if Pos:,@[1]\n",
      "   8  10   2   3  | NPS->JJ if Pos:NN@[1]\n",
      "   8   8   0   0  | TO->IN if Pos:,@[1]\n",
      "   8  12   4   0  | VBD->VBN if Pos:HVZ@[-3,-2,-1]\n",
      "   8   8   0   0  | ,-HL->, if Pos:NN@[1,2,3]\n",
      "   8   8   0   0  | VBD->VBN if Pos:BER@[-2] & Pos:RB@[-1]\n",
      "   8   8   0   0  | DT->CS if Word:except@[-1]\n",
      "   8   8   0   0  | ABN->QL if Word:right@[1]\n",
      "   8   8   0   0  | IN->TO if Word:see@[1]\n",
      "   8   8   0   0  | NN->VB if Pos:TO@[-1]\n",
      "   8   8   0   0  | JJ->JJ-TL if Word:League@[1]\n"
     ]
    }
   ],
   "source": [
    "# CODE SUBMISSION ANSWER HERE (Double click to edit)\n",
    "unigram_tagger = UnigramTagger(brown_sents_train, backoff=backoff)\n",
    "bigram_tagger = BigramTagger(brown_sents_train, backoff=backoff)\n",
    "\n",
    "templates = brill24()\n",
    "brill_trainer = BrillTaggerTrainer(initial_tagger=bigram_tagger,\n",
    "                                                 templates=templates,\n",
    "                                                 trace=3)\n",
    "brill_tagger = brill_trainer.train(brown_sents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f82caa52-98ed-416a-8d07-62ebd378b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger performance on test set:\n",
      "Unigram Tagger's accuracy 0.8754534941803576\n",
      "Bigram Tagger's accuracy 0.8067550276099437\n",
      "Brill Tagger's accuracy 0.8271857016757559\n"
     ]
    }
   ],
   "source": [
    "unigram_tags = [unigram_tagger.tag(sentence) for sentence in brown_sents_test]\n",
    "bigram_tags = [bigram_tagger.tag(sentence) for sentence in brown_sents_test]\n",
    "brill_tags = [brill_tagger.tag(sentence) for sentence in brown_sents_test]\n",
    "\n",
    "print(\"Tagger performance on test set:\")\n",
    "print(\"Unigram Tagger's accuracy\", unigram_tagger.accuracy(brown_sents_test))\n",
    "print(\"Bigram Tagger's accuracy\", bigram_tagger.accuracy(brown_sents_test))\n",
    "print(\"Brill Tagger's accuracy\", brill_tagger.accuracy(brown_sents_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f97769-7af9-4f27-afab-3353272230de",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "Discuss the scores of your taggers. Which one performs better, and why?\n",
    "\n",
    "The UnigramTagger performed better than the other taggers. One possible reason is that in the training data, there may be few or no occurrences of specific word pairs, which makes it harder for the BigramTagger to generalize. As a result, the BigramTagger might fail to assign the correct tag in the test set when encountering unfamiliar or rare bigram combinations.??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0cb08-2647-4f55-948f-a65d4206a801",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 100-150 words\n",
    "\n",
    "The UnigramTagger performed better than the other taggers. One possible reason is that in the training data, there may be few or no occurrences of specific word pairs, which makes it harder for the BigramTagger to generalize. As a result, the BigramTagger might fail to assign the correct tag in the test set when encountering unfamiliar or rare bigram combinations. (??)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627484d-9018-4846-8574-c46ea15f0b67",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "Discuss ideas for improving the implementations and the quality of the taggers. You are not required to implement the improvement ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a3013-e28e-4acc-a5c3-5e1fd7a80005",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 100-250 words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aded10e-f01a-44c0-8f78-b9318426fd33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f867e-c1f9-4d71-be99-3204b3658f85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3 - Unigram and Bigram Taggers (pen and paper):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9b940-bb2a-4e1e-8353-a1bd7d0a736f",
   "metadata": {},
   "source": [
    "**Training data:**\n",
    "\n",
    "His [PRP] raise [NN] was [VB] five [CD] dollars [NN] . [SYM]\n",
    "We [PRP] usually [RB] get [VB] a [DT] raise [NN] at [IN] the [DT] start [NN] of [IN] the [DT] year [NN] . [SYM]\n",
    "A [DT] major [JJ] success [NN] helped [VB] to [TO] raise [VB] our [PRP] spirits [NN] . [SYM]\n",
    "\n",
    "\n",
    "\n",
    "**Test sentence:**\n",
    "\n",
    "It [PRP] looks [VB] like [CC] a [DT] fine [JJ] place [NN] to [TO] raise [NN or VB?] children [NN] . [SYM]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd51b7-800a-4fc1-a69b-4faa049a82c9",
   "metadata": {},
   "source": [
    "### Part 1: Unigram Tagger\n",
    "\n",
    "Given the training data, determine the most likely tag for the word \"raise\" in the test sentence, using Unigram tagging method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc77aa4-789e-4483-8656-0a2296e8058a",
   "metadata": {},
   "source": [
    "<font color='ff000000'>\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - write down the steps for calculating the most likely POS tag</font>\n",
    "\n",
    "*Assigns the most frequent tag to word based on the training data. So in this case for the word \"raise\", unigram tagger would assign \"NN\" to the word \"raise\" in test sentence, which is incorrect. \n",
    "\n",
    "p(VB|raise) = 1/3 or p(NN|race)= 2/3\n",
    "\n",
    "-Assigned NN which is wrong. \n",
    "\n",
    "!But we should look at the context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae00231-a0cd-44c6-97b0-fa444e237d57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 2 - Bigram Tagger:\n",
    "\n",
    "Given the training data (in Task 3), determine the most likely tag for the word \"raise\" in the test sentence, using Bigram tagging method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba02ef-c4e1-4ede-aa3a-b03c92aa5f33",
   "metadata": {},
   "source": [
    "<font color='ff000000'>\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - write down the steps for calculating the most likely POS tag</font\n",
    "\n",
    "*Looks at the preceding word.\n",
    "\n",
    "p(VB|raise) = p(VB|TO) * p(raise|VB)= 1 * 1/4 = 1/4    the probability of assigning VB to \"raise\" =  TO preceding-coming before VB  * raise-tagged as VB / total VB in training sentence\n",
    "\n",
    "P(NN|raise) =  p(NN|TO)  * p(raise|NN) = 0 * 2/7 = 0   the probability of assigning NN to \"raise\" = TO preceding-coming before NN  * raise-tagged as NN / total NN in the training sentence\n",
    "\n",
    "-Assigned VB to \"raise\", which is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f649cb9-cd57-43cd-b27c-3cef5fbf9115",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4717fe7-44f7-4446-a698-e3564228a265",
   "metadata": {},
   "source": [
    "#### Submitting your results:\n",
    "\n",
    "To submit your results, please:\n",
    "\n",
    "- save this file, i.e., `ex??_assignment.ipynb`.\n",
    "- if you reference any external files (e.g., images), please create a zip or rar archieve and put the notebook files and all referenced files in there.\n",
    "- login to ILIAS and submit the `*.ipynb` or archive for the corresponding assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b6bc4-1fee-4efe-9a1a-657ec6eab062",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "    \n",
    "- Do not copy any code from the Internet. In case you want to use publicly available code, please, add the reference to the respective code snippet.\n",
    "- Check your code compiles and executes, even after you have restarted the Kernel.\n",
    "- Submit your written solutions and the coding exercises within the provided spaces and not otherwise.\n",
    "- Write the names of your partner and your name in the top section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df879020-f06b-4172-9ac4-8e0f7bda365a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (docana)",
   "language": "python",
   "name": "docana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
