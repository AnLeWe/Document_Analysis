{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83cf750-33a5-4457-9a8c-4b13b4c44cd4",
   "metadata": {},
   "source": [
    "## Document Analysis: Computational Methods - Summer Term 2025\n",
    "### Lectures: Jun.-Prof. Dr. Andreas Spitz\n",
    "### Tutorials: Julian Schelb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675e5c2-c60f-43e9-9076-a3bf7a36ffa1",
   "metadata": {},
   "source": [
    "# Exercise 02\n",
    "\n",
    "You will learn about:\n",
    "    \n",
    "- The Brown Corpus\n",
    "- Part of Speech (POS) tagging\n",
    "- Unigram and Bigram tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd621e3-4f8e-4fbb-88e2-8910fca352ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b77e4-4f2e-4441-85e3-5a007faa73c7",
   "metadata": {},
   "source": [
    "## Task 1 - The Brown Corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbabdb-28ad-414c-b4fd-5d8e244e793a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42de428-53b6-4a0e-8311-3ecd0dafa6d7",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "In the following, we will use the _Brown Corpus_. In one or two sentences, describe what the _Brown Corpus_ is and how it can be used for POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7b3de-dd67-4ca1-80f4-3a2d80b0e207",
   "metadata": {},
   "source": [
    "**EXAMPLE SOLUTION**\n",
    "\n",
    "The Brown Corpus (Brown University Standard Corpus of Present-Day American English) is a general corpus (a reference text collection) originally compiled by Henry Kuƒçera and W. Nelson Francis in the 1960s for linguistics research. It consists of 500 American English texts, totaling roughly one million words, which all have been manually tagged and can be used as ground truth for POS tagging. Compared to contemporary corpi (e.g., Corpus of Contemporary American English or International Corpus of English), it is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09200db1-e168-4173-a85e-92c5630ddabc",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "We start by analyzing which tags occur in the brown corpus. For this, you should extract the `tagged_words` first. Then\n",
    "\n",
    "1. List the first 20 entries and\n",
    "2. then list the ten most common tags in the category `news`.\n",
    "\n",
    "In the lecture, we use the Brown Corpus POS tags (default, i.e., `tagset=None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48b5e71-e2ed-4b57-af65-f352d0873632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/julianschelb/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfca02e-0ed1-46ca-a0e7-b37b819d7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE SOLUTION\n",
    "\n",
    "brown_news_tagged = brown.tagged_words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df224b-e3de-47ec-9f0b-2d9a81520e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_news_tagged[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b802b-9ba5-4449-a816-34985309ce27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 13162),\n",
       " ('IN', 10616),\n",
       " ('AT', 8893),\n",
       " ('NP', 6866),\n",
       " (',', 5133),\n",
       " ('NNS', 5066),\n",
       " ('.', 4452),\n",
       " ('JJ', 4392),\n",
       " ('CC', 2664),\n",
       " ('VBD', 2524)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
    "tags.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02186ccd-c25c-4788-8cd3-d4bc636ad93e",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "In the previous part, you should get ten different POS tags. For each tag, what does it stand for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9703d-5e72-40d9-a220-ccd2bac2e5f9",
   "metadata": {},
   "source": [
    "**EXAMPLE SOLUTION**\n",
    "\n",
    "- **NN** - Singular or mass noun: *apple*\n",
    "- **IN** - Preposition: *under*\n",
    "- **AT** - Article (a, the, no): *the*\n",
    "- **NP** - Proper noun or part of name phrase: *London*\n",
    "- **`,`** - Comma: *`Used to separate elements in a sentence, such as clauses or items in a list.`*\n",
    "- **NNS** - Plural noun: *apples*\n",
    "- **`.`** - Sentence closer: *`Marks the end of a sentence, like a period.`*\n",
    "- **JJ** - Adjective: *green*\n",
    "- **CC** - Coordinating conjunction (and, or): *and*\n",
    "- **VBD** - Verb, past tense: *walked*\n",
    "\n",
    "\n",
    "\n",
    "see also http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM#bc6 or https://varieng.helsinki.fi/CoRD/corpora/BROWN/tags.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6adb6-1284-4ad3-87b7-b2d3ba5ffbea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab5f72-7433-4338-8f7f-2ab002c03074",
   "metadata": {},
   "source": [
    "## Task 2 - POS Tagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c060a2-003d-47a7-a929-e08c940b7389",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "Use a Unigram tagger, trained on the Brown corpus, to tag the example sentence from the Penn treebank (see also https://www.nltk.org/_modules/nltk/corpus/reader/tagged.html)\n",
    "\n",
    "For which words does it completely fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a5acd4-6ab3-494b-a08b-6992b2a0919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/julianschelb/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/julianschelb/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d492dec-b7b7-4c86-92e8-f3873221dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_test = list(nltk.corpus.treebank.words()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7eed60-2a56-4ce9-a0af-cae2eccb987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pierre, NP), \n",
      "(Vinken, None), \n",
      "(,, ,), \n",
      "(61, CD), \n",
      "(years, NNS), \n",
      "(old, JJ), \n",
      "(,, ,), \n",
      "(will, MD), \n",
      "(join, VB), \n",
      "(the, AT), \n",
      "(board, NN), \n",
      "(as, CS), \n",
      "(a, AT), \n",
      "(nonexecutive, None), \n",
      "(director, NN), \n",
      "(Nov., NP), \n",
      "(29, CD), \n",
      "(., .), \n",
      "(Mr., NP), \n",
      "(Vinken, None), \n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE SOLUTION\n",
    "\n",
    "brown_sents_tagged = brown.tagged_sents()\n",
    "tagger_unigram = UnigramTagger(brown_sents_tagged)\n",
    "\n",
    "for tok, tag in tagger_unigram.tag(treebank_test):\n",
    "    print(f\"({tok}, {tag}), \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b2244-39c2-4139-90c2-f63b8f1004e0",
   "metadata": {},
   "source": [
    "**EXAMPLE SOLUTION**\n",
    "\n",
    "It fails for Vinken and nonexecutive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49def5-67ab-40e2-a795-28e96acddb1f",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Compare the tags with the reference tags from the Penn treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170ceed1-3121-4580-b277-223c381167ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pierre, NNP), \n",
      "(Vinken, NNP), \n",
      "(,, ,), \n",
      "(61, CD), \n",
      "(years, NNS), \n",
      "(old, JJ), \n",
      "(,, ,), \n",
      "(will, MD), \n",
      "(join, VB), \n",
      "(the, DT), \n",
      "(board, NN), \n",
      "(as, IN), \n",
      "(a, DT), \n",
      "(nonexecutive, JJ), \n",
      "(director, NN), \n",
      "(Nov., NNP), \n",
      "(29, CD), \n",
      "(., .), \n",
      "(Mr., NNP), \n",
      "(Vinken, NNP), \n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE SOLUTION\n",
    "\n",
    "treebank_reference = list(nltk.corpus.treebank.tagged_words()[0:20])\n",
    "for tok, tag in treebank_reference:\n",
    "    print(f\"({tok}, {tag}), \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13abc46-aebe-41d1-ba16-29edc535e942",
   "metadata": {},
   "source": [
    "**EXAMPLE SOLUTION**\n",
    "\n",
    "The differences are:\n",
    "\n",
    "```\n",
    "Pierre (NNP instead of NP)\n",
    "Vinken (NNP instead of None)\n",
    "the (DT instead of AT)\n",
    "as (IN instead of CS) \n",
    "a (DT instead of AT)\n",
    "nonexecutive (JJ instead of None)\n",
    "Nov. (NNP instead of NP)\n",
    "Mr. (NNP instead of NP)\n",
    "Vinken (NNP instead of None)\n",
    "```\n",
    "\n",
    "We can see that the tagging has troubles with names, special words and sometimes makes mistakes within the same word class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9781814-a8ac-4d0f-a884-189cb65492cd",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Now train \n",
    " 1. a Unigram tagger,\n",
    " 2. a Bigram tagger,\n",
    " 3. and a Brill tagger (using rule brill24)\n",
    " \n",
    "with a subset of the Brown Corpus. This might take 1-2 minutes.\n",
    "\n",
    "Then, validate and compare their performance on a different subset of the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f019a5-0973-4268-bc65-e819a1722be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.tag.brill_trainer import BrillTaggerTrainer\n",
    "from nltk.tag.brill import brill24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdedb37-aadf-4d0e-8078-019751b0c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cutoff = 20000\n",
    "brown_sents_train = brown.tagged_sents()[0:n_cutoff] # training corpus\n",
    "brown_sents_test = brown.tagged_sents()[n_cutoff:] # reference corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2e1d093-a53c-45dc-b4c5-78b20afb3bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBL train (fast) (seqs: 20000; tokens: 430477; tpls: 24; min score: 2; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 172575 useful rules.\n",
      "Selecting rules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/f_tn4q4x449826sdsnd1zv6c0000gn/T/ipykernel_40517/1215878508.py:16: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('unigram', tagger_unigram.evaluate(brown_sents_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram 0.8615109858152631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/f_tn4q4x449826sdsnd1zv6c0000gn/T/ipykernel_40517/1215878508.py:17: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('bigram', tagger_bigram.evaluate(brown_sents_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram 0.8776540785395127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/f_tn4q4x449826sdsnd1zv6c0000gn/T/ipykernel_40517/1215878508.py:18: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('brill', tagger_brill.evaluate(brown_sents_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brill 0.8854122332236234\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE SOLUTION\n",
    "\n",
    "# Unigram\n",
    "tagger_unigram = UnigramTagger(brown_sents_train)\n",
    "\n",
    "# Bigram\n",
    "tagger_bigram = BigramTagger(brown_sents_train, backoff=tagger_unigram)\n",
    "\n",
    "# Brill\n",
    "trainer_brill = BrillTaggerTrainer(tagger_bigram, brill24(), trace=1)\n",
    "tagger_brill = trainer_brill.train(brown_sents_train, max_rules=20)\n",
    "\n",
    "#\n",
    "# Evaluate\n",
    "#\n",
    "print('unigram', tagger_unigram.evaluate(brown_sents_test))\n",
    "print('bigram', tagger_bigram.evaluate(brown_sents_test))\n",
    "print('brill', tagger_brill.evaluate(brown_sents_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f97769-7af9-4f27-afab-3353272230de",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "Discuss the scores of your taggers. Which one performs better, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0cb08-2647-4f55-948f-a65d4206a801",
   "metadata": {},
   "source": [
    "**EXAMPLE SOLUTION**\n",
    "\n",
    "We achieved a score of 86.2% accuracy for the Unigram tagger, 87.8% accuracy for the Bigram tagger, and 89.1% accuracy for the Brill tagger. These results match our expectations: The more information (more context) the tagger takes into account, the better the performance. Nevertheless, one might expect the difference between unigram and the other taggers to be bigger since unigram does not take into account any context, but the others do. Maybe this observation would change if the training set is more complex and thus more bigrams and trigrams of tags in the test text also occur in the training text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8415c3",
   "metadata": {},
   "source": [
    "**Brill Tagger:**\n",
    "1. **Initial Tagging**: Start with a simple baseline model, such as a unigram tagger, to provide a preliminary tagging based on the most frequent tags.\n",
    "2. **Learning Transformation Rules**: Analyze tagging errors to automatically generate potential transformation rules that suggest modifying one part-of-speech tag to another under specific conditions.\n",
    "3. **Rule Selection and Optimization**: Evaluate and rank the generated rules by their effectiveness in improving tagging accuracy, selecting the most beneficial rules for application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627484d-9018-4846-8574-c46ea15f0b67",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "Discuss ideas for improving the implementations and the quality of the taggers. You are not required to implement the improvement ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a3013-e28e-4acc-a5c3-5e1fd7a80005",
   "metadata": {},
   "source": [
    "**EXAMPLE SOLUTION**\n",
    "\n",
    "- In terms of quality, using a larger training corpus will almost certainly improve the accuracy, although the increase might only be marginal.\n",
    "- A different approach would be to add hard-coded rules that account for specific cases, which often produce errors when using the statistical n-gram approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aded10e-f01a-44c0-8f78-b9318426fd33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f867e-c1f9-4d71-be99-3204b3658f85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3 - Unigram and Bigram Taggers (pen and paper):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9b940-bb2a-4e1e-8353-a1bd7d0a736f",
   "metadata": {},
   "source": [
    "**Training data:**\n",
    "\n",
    "His [PRP] raise [NN] was [VB] five [CD] dollars [NN] . [SYM]\n",
    "We [PRP] usually [RB] get [VB] a [DT] raise [NN] at [IN] the [DT] start [NN] of [IN] the [DT] year [NN] . [SYM]\n",
    "A [DT] major [JJ] success [NN] helped [VB] to [TO] raise [VB] our [PRP] spirits [NN] . [SYM]\n",
    "\n",
    "\n",
    "\n",
    "**Test sentence:**\n",
    "\n",
    "It [PRP] looks [VB] like [CC] a [DT] fine [JJ] place [NN] to [TO] raise [NN or VB?] children [NN] . [SYM]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd51b7-800a-4fc1-a69b-4faa049a82c9",
   "metadata": {},
   "source": [
    "### Part 1: Unigram Tagger\n",
    "\n",
    "Given the training data, determine the most likely tag for the word \"raise\" in the following sentence, using Unigram tagging method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc77aa4-789e-4483-8656-0a2296e8058a",
   "metadata": {},
   "source": [
    "**EXAMPLE SOLUTION**\n",
    "\n",
    "**Naive solution**\n",
    "\n",
    "p(k=NN|raise) = 2 / 3\n",
    "\n",
    "p(k=VB|raise) = 1 / 3\n",
    "\n",
    "**Consider frequence (Bayes rule), so apply the adapted formula:**\n",
    "\n",
    "\n",
    "p(NN) * p(raise|NN) = 7/27 * 2/7 = 2/27 -- most likely tag, but it would be wrong\n",
    "\n",
    "p(VB) * p(raise|VB) = 4/27 * 1/4 = 1/27\n",
    "\n",
    "\n",
    "**Granted: If we plug in all the numbers (including the marginal probability), we indeed end up at the top probabilities**\n",
    "\n",
    "\n",
    "**Bayes Theorem**\n",
    "\n",
    "**Bayes' Theorem for Part-of-Speech Tagging**:\n",
    "`p(k=Tag|word) = p(word|k=Tag) * p(Tag) / p(word)`\n",
    "\n",
    "**Components**:\n",
    "1. **Posterior Probability, `p(k=Tag|word)`**:\n",
    "   - Probability of the tag given the word.\n",
    "2. **Likelihood, `p(word|k=Tag)`**:\n",
    "   - Probability of observing the word given the tag.\n",
    "3. **Prior Probability, `p(Tag)`**:\n",
    "   - Overall probability of the tag in the corpus.\n",
    "4. **Marginal Probability, `p(word)`**:\n",
    "   - Overall probability of observing the word across all tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae00231-a0cd-44c6-97b0-fa444e237d57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 2 - Bigram Tagger:\n",
    "\n",
    "Given the training data (in Task 2), determine the most likely tag for the word \"raise\" in the following sentence, using Bigram tagging method:\n",
    "\n",
    "It [PRP] looks [VB] like [CC] a [DT] fine [JJ] place [NN] to [TO] raise [NN or VB?] children [NN] . [SYM]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba02ef-c4e1-4ede-aa3a-b03c92aa5f33",
   "metadata": {},
   "source": [
    "**EXAMPLE SOLUTION**\n",
    "\n",
    "p(NN|TO) * p(raise|NN) = 0/1 * 2/7 = 0\n",
    "\n",
    "p(VB|TO) * p(raise|VB) = 1/1 * 1/4 = 1/4 -- most likely tag, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f649cb9-cd57-43cd-b27c-3cef5fbf9115",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4717fe7-44f7-4446-a698-e3564228a265",
   "metadata": {},
   "source": [
    "#### Submitting your results:\n",
    "\n",
    "To submit your results, please:\n",
    "\n",
    "- save this file, i.e., `ex??_assignment.ipynb`.\n",
    "- if you reference any external files (e.g., images), please create a zip or rar archieve and put the notebook files and all referenced files in there.\n",
    "- login to ILIAS and submit the `*.ipynb` or archive for the corresponding assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b6bc4-1fee-4efe-9a1a-657ec6eab062",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "    \n",
    "- Do not copy any code from the Internet. In case you want to use publicly available code, please, add the reference to the respective code snippet.\n",
    "- Check your code compiles and executes, even after you have restarted the Kernel.\n",
    "- Submit your written solutions and the coding exercises within the provided spaces and not otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df879020-f06b-4172-9ac4-8e0f7bda365a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc49a7-8eae-45ba-913b-fc84efeaa3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
